{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109
    },
    "colab_type": "code",
    "id": "BHU2up3V1UU6",
    "outputId": "0967fd8d-67b3-4687-9522-6166629366af"
   },
   "outputs": [],
   "source": [
    "!pip install sklearn_crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "x8TaSlPS1bOh",
    "outputId": "449eb30d-d2a7-463e-fbfb-59ea2107d86c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2002 to\n",
      "[nltk_data]     C:\\Users\\ashis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2002 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import conll2002\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import Perceptron, SGDClassifier, RidgeClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import sklearn_crfsuite\n",
    "import sklearn_crfsuite.metrics\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import nltk\n",
    "from nltk.classify import MaxentClassifier\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('conll2002')\n",
    "\n",
    "regex = re.compile(\n",
    "        r'^(?:http|ftp)s?://'  # http:// or https://\n",
    "        r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|'  # domain...\n",
    "        r'localhost|'  # localhost...\n",
    "        r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})'  # ...or ip\n",
    "        r'(?::\\d+)?'  # optional port\n",
    "        r'(?:/?|[/?]\\S+)$', re.IGNORECASE)\n",
    "\n",
    "def wordshape(text):\n",
    "\n",
    "    t1 = re.sub('[A-Z]', 'X',text)\n",
    "    t2 = re.sub('[a-z]', 'x', t1)\n",
    "    return re.sub('[0-9]', 'd', t2)\n",
    "\n",
    "def getfeats(word, o):\n",
    "    \"\"\" This takes the word in question and\n",
    "    the offset with respect to the instance\n",
    "    word \"\"\"\n",
    "    features = [\n",
    "        (str(o) + 'word', word)\n",
    "        # TODO: add more features here.\n",
    "    ]\n",
    "    return features\n",
    "\n",
    "def gettag(tag, o):\n",
    "    features = [ (str(o) +\"tag\", tag) ]\n",
    "    return features\n",
    "\n",
    "def gethyphen(word, o):\n",
    "    if('-' in word):\n",
    "        features = [(str(o) +\"hyphen\", 1)]\n",
    "    else:\n",
    "        features = [(str(o) +\"hyphen\", 0)]\n",
    "    return features\n",
    "    \n",
    "def capletter(word, o):\n",
    "    if(word[0].isupper):\n",
    "        features = [(str(o) +\"first_upper\", 1)]\n",
    "    else:\n",
    "        features = [(str(o) +\"first_upper\", 0)]\n",
    "    return features\n",
    "\n",
    "def noun_suffix(word, o):\n",
    "    if(word.endswith('o') or word.endswith('or') or word.endswith('a') or word.endswith('ora')):\n",
    "        features = [(str(o) +\"common_suffix\", 1)]\n",
    "    else:\n",
    "        features = [(str(o) +\"common_suffix\", 0)]\n",
    "    return features\n",
    "\n",
    "def get_wordshape(word, o):\n",
    "    feature = [(str(o) +\"word_shape\", wordshape(word))]\n",
    "    return feature\n",
    "\n",
    "def all_upper(word, o):\n",
    "    if(word.isupper()):\n",
    "        return [(str(o) +\"all_upper\", 1)]\n",
    "    else:\n",
    "        return [(str(o) +\"all_upper\", 0)]\n",
    "\n",
    "def all_lower(word, o):\n",
    "    if(word.islower()):\n",
    "        return [(str(o) +\"all_lower\", 1)]\n",
    "    else:\n",
    "        return [(str(o) +\"all_lower\", 0)]\n",
    "\n",
    "def has_apostrophe(word, o):\n",
    "    if(\"'\" in word):\n",
    "        return [(str(o) +\"apostrophe\", 1)]\n",
    "    else:\n",
    "        return [(str(o) +\"apostrophe\", 0)]\n",
    "\n",
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def special_characters(word, o):\n",
    "    if(isEnglish(word)):\n",
    "        return [(str(o) +\"special_characters\", 0)]\n",
    "    else:\n",
    "        return [(str(o) +\"special_characters\", 1)]\n",
    "\n",
    "def onlynum(word, o):\n",
    "    if(word.isdigit()):\n",
    "        return [(str(o) +\"onlynum\", 1)]\n",
    "    else:\n",
    "        return [(str(o) +\"onlynum\", 0)]\n",
    "\n",
    "def contains_num(word, o):\n",
    "    if(any(char.isdigit() for char in word)):\n",
    "        return [(str(o) +\"contains_num\", 1)]\n",
    "    else:\n",
    "        return [(str(o) +\"contains_num\", 0)]\n",
    "\n",
    "def ending_fullstop(word, o):\n",
    "    if(word[-1] == '.'):\n",
    "        return [(str(o) +\"fullstop\", 1)]\n",
    "    else:\n",
    "        return [(str(o) +\"fullstop\", 0)]\n",
    "\n",
    "def minlen(word, o):\n",
    "    if(len(word)>=2):\n",
    "        return [(str(o) +\"minlen\", 1)]\n",
    "    else:\n",
    "        return [(str(o) +\"minlen\", 0)]\n",
    "\n",
    "def punctuation(word, o):\n",
    "    for i in word: \n",
    "      if i in string.punctuation: \n",
    "        return [(str(o) +\"punctuation\", 1)]\n",
    "    return [(str(o) +\"punctuation\", 0)]\n",
    "\n",
    "def all_punctuation(word, o):\n",
    "    count = 0\n",
    "    for i in word: \n",
    "      if i in string.punctuation: \n",
    "        count = count +1\n",
    "    if(count == len(word)):\n",
    "        return [(str(o) +\"punctuation\", 1)]\n",
    "    else:\n",
    "        return [(str(o) +\"punctuation\", 0)]\n",
    "\n",
    "def is_stopword(word, o):\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    if(word in stop_words):\n",
    "        return([(str(o) +\"is_stop\", 1)])\n",
    "    else:\n",
    "        return([(str(o) +\"is_stop\", 0)])\n",
    "\n",
    "\n",
    "def isRomanNumeral(word, o):\n",
    "    numeral = word.upper()\n",
    "    validRomanNumerals = [\"M\", \"D\", \"C\", \"L\", \"X\", \"V\", \"I\"]\n",
    "    for letters in numeral:\n",
    "        if letters not in validRomanNumerals:\n",
    "            return ([(str(o) +\"is_roman\", 0)])\n",
    "\n",
    "    return ([(str(o) + \"is_roman\", 1)])\n",
    "\n",
    "\n",
    "def contains_dots(word, o):\n",
    "    if word.find('.')==-1:\n",
    "        return ([(str(o) + \"has_dot\", 0)])\n",
    "\n",
    "    return ([(str(o) + \"has_dot\", 1)])\n",
    "\n",
    "\n",
    "def single_char(word, o):\n",
    "    if len(word)==1:\n",
    "        return ([(str(o) + \"is_char\", 1)])\n",
    "\n",
    "    return ([(str(o) + \"is_char\", 0)])\n",
    "\n",
    "def is_url(word, o):\n",
    "    if re.match(regex, word) is not None:\n",
    "        return ([(str(o) + \"is_url\", 1)])\n",
    "    return ([(str(o) + \"is_url\", 0)])\n",
    "\n",
    "\n",
    "def word2features(sent, i):\n",
    "    \"\"\" The function generates all features\n",
    "    for the word at position i in the\n",
    "    sentence.\"\"\"\n",
    "    features = []\n",
    "    # the window around the token\n",
    "    for o in [-4,-3, -2,-1,0,1,2, 3, 4]:\n",
    "        if i+o >= 0 and i+o < len(sent):\n",
    "            word = sent[i+o][0]\n",
    "            tag = sent[i+o][1]\n",
    "            featlist = getfeats(word, o)\n",
    "            features.extend(featlist)\n",
    "            featlist = gettag(tag, o)\n",
    "            features.extend(featlist)\n",
    "\n",
    "            featlist = gethyphen(word, o)\n",
    "            features.extend(featlist)\n",
    "\n",
    "            featlist = capletter(word, o)\n",
    "            features.extend(featlist)\n",
    "\n",
    "            featlist = noun_suffix(word, o)\n",
    "            features.extend(featlist)\n",
    "\n",
    "            featlist = get_wordshape(word, o)\n",
    "            features.extend(featlist)\n",
    "\n",
    "            featlist = all_upper(word, o)\n",
    "            features.extend(featlist)\n",
    "\n",
    "            featlist = all_lower(word, o)\n",
    "            features.extend(featlist)\n",
    "\n",
    "            featlist = has_apostrophe(word, o)\n",
    "            features.extend(featlist)\n",
    "\n",
    "            featlist = special_characters(word, o)\n",
    "            features.extend(featlist)\n",
    "\n",
    "            featlist = onlynum(word, o)\n",
    "            features.extend(featlist)\n",
    "\n",
    "            featlist = contains_num(word, o)\n",
    "            features.extend(featlist)\n",
    "\n",
    "            featlist = ending_fullstop(word, o)\n",
    "            features.extend(featlist)\n",
    "\n",
    "            featlist = isRomanNumeral(word, o)\n",
    "            features.extend(featlist)\n",
    "\n",
    "            featlist = contains_dots(word, o)\n",
    "            features.extend(featlist)\n",
    "\n",
    "            featlist = single_char(word, o)\n",
    "            features.extend(featlist)\n",
    "\n",
    "            featlist = is_url(word, o)\n",
    "            features.extend(featlist)\n",
    "\n",
    "    \n",
    "    word = sent[i][0]\n",
    "    tag = sent[i][1]\n",
    "\n",
    "    features.extend([(\"word_lower\", word.lower())])\n",
    "\n",
    "    features.extend([(\"word_len\", len(word))])\n",
    "\n",
    "    if (i == 0):\n",
    "        features.extend([(\"firstword\", 1)])\n",
    "    else:\n",
    "        features.extend([(\"firstword\", 0)])\n",
    "\n",
    "    features.extend([(\"bias\", 1)])\n",
    "    \n",
    "    return dict(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ucnQ_s6k8S4y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "cbmP69XL-Khv",
    "outputId": "109ba715-6d10-41aa-d8f5-5bf9e14ac0df"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rj-1nnfV1cpb"
   },
   "outputs": [],
   "source": [
    "train = ConllCorpusReader('./CoNLL-2003', 'eng.train', ['words', 'pos', 'ignore', 'chunk'])\n",
    "dev = ConllCorpusReader('./CoNLL-2003', 'eng.testa', ['words', 'pos', 'ignore', 'chunk'])\n",
    "test = ConllCorpusReader('./CoNLL-2003', 'eng.testb', ['words', 'pos', 'ignore', 'chunk'])\n",
    "\n",
    "train_sents = list(train.iob_sents())\n",
    "dev_sents = list(dev.iob_sents())\n",
    "test_sents = list(test.iob_sents())\n",
    "\n",
    "unannotated_dataset = list(train.iob_sents())\n",
    "train_ua = ConllCorpusReader('./CoNLL-2003', 'eng.train', ['words'])\n",
    "train_ua = list(train_ua.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sents = list(conll2002.iob_sents('esp.train'))\n",
    "# dev_sents = list(conll2002.iob_sents('esp.testa'))\n",
    "# test_sents = list(conll2002.iob_sents('esp.testb'))\n",
    "\n",
    "# unannotated_dataset = list(conll2002.iob_sents('esp.train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "V8LTHv_SUuZt",
    "outputId": "373bec0d-efc2-4937-ed7b-96c0ae5e9ffb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14987"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PWr73Tzu1e3C"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mrvafjm_1i-F"
   },
   "outputs": [],
   "source": [
    "X_train = []\n",
    "train_labels = []\n",
    "\n",
    "for sent in unannotated_dataset:\n",
    "    unannotated_dataset_feats = []\n",
    "    unannotated_dataset_labels = []\n",
    "\n",
    "    for i in range(len(sent)):\n",
    "        feats = word2features(sent, i)   \n",
    "        unannotated_dataset_feats.append(feats)\n",
    "        unannotated_dataset_labels.append(sent[i][-1])\n",
    "\n",
    "    X_train.append(unannotated_dataset_feats)\n",
    "    train_labels.append(unannotated_dataset_labels)\n",
    "\n",
    "ind_select=np.array([len(y)>0 for y in X_train])\n",
    "X_train = np.array(X_train)[ind_select]\n",
    "train_labels = np.array(train_labels)[ind_select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NXzVK3UuEzmI"
   },
   "outputs": [],
   "source": [
    "test_feats = []\n",
    "test_labels = []\n",
    "\n",
    "# for sent in test_sents:\n",
    "#     for i in range(len(sent)):\n",
    "#         feats = word2features(sent, i)\n",
    "#         test_feats.append(feats)\n",
    "#         test_labels.append(sent[i][-1])\n",
    "\n",
    "\n",
    "for sent in test_sents:\n",
    "    test_sents_feats = []\n",
    "    test_sents_labels = []\n",
    "\n",
    "    for i in range(len(sent)):\n",
    "        feats = word2features(sent, i)   \n",
    "        test_sents_feats.append(feats)\n",
    "        test_sents_labels.append(sent[i][-1])\n",
    "\n",
    "    test_feats.append(test_sents_feats)\n",
    "    test_labels.append(test_sents_labels)\n",
    "\n",
    "ind_select=np.array([len(y)>0 for y in test_feats])\n",
    "test_feats = np.array(test_feats)[ind_select]\n",
    "test_labels = np.array(test_labels)[ind_select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Jki8KCS2Cvi"
   },
   "outputs": [],
   "source": [
    "initial_size_of_training_set = 20\n",
    "\n",
    "training_dataset_feats = {}\n",
    "training_dataset_labels = {}\n",
    "\n",
    "all_length=np.array([len(X_train[i]) for i in range(len(X_train))])\n",
    "\n",
    "indices=all_length.argsort()[-initial_size_of_training_set:][::-1]\n",
    "\n",
    "\n",
    "training_dataset_feats = list(X_train[indices])\n",
    "training_dataset_labels =  list(train_labels[indices])\n",
    "\n",
    "X_train = np.delete(X_train, indices)\n",
    "train_labels = np.delete(train_labels, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "njVJWUarWNmP"
   },
   "outputs": [],
   "source": [
    "X_train_random=X_train.copy()\n",
    "train_labels_random=train_labels.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YHPDjAujV-TP"
   },
   "outputs": [],
   "source": [
    "initial_training_dataset_feats=training_dataset_feats.copy()\n",
    "initial_training_dataset_labels=training_dataset_labels.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "dGqU5R4d28xy",
    "outputId": "ba5ccc1c-591e-4e67-a08b-ef7e0266f5e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14021,), (14021,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8-40w38RAk-X"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j3nzy5kXbWkP"
   },
   "outputs": [],
   "source": [
    "\n",
    "# X_train = X_train[0:10000]\n",
    "# train_labels = train_labels[0:10000]\n",
    "\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zq9I9Gd69E-T"
   },
   "outputs": [],
   "source": [
    "# prediction_of_classifier[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mJA0CxzR7KA2"
   },
   "outputs": [],
   "source": [
    "# crf.predict_marginals_single(X_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "iYY0wzA3AGZ4",
    "outputId": "26f56aa9-6ca2-4768-f948-76cb6d389621"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIZE OF TRAINING DATASET  20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "labels_predicted_for_unannotated_data = []\n",
    "\n",
    "print(\"SIZE OF TRAINING DATASET \",len(training_dataset_feats))\n",
    "crf.fit(training_dataset_feats,training_dataset_labels)\n",
    "prediction_of_classifier=crf.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AU6enffjB3tV"
   },
   "outputs": [],
   "source": [
    "# def get_uncertainy_of_predicted_values(X_train,crf,prediction_of_classifier,n):\n",
    "#     uncertainity_value_all_predictions=[]\n",
    "#     for sent in range(len(X_train)):\n",
    "#         uncertainity_value=0\n",
    "#         li=[]\n",
    "#         for words in range(len(X_train[sent])):\n",
    "#             li.append(crf.predict_marginals_single(X_train[sent])[words][prediction_of_classifier[sent][words]])\n",
    "#         li=np.array(li)\n",
    "#         ind=li.argsort()[:n]\n",
    "\n",
    "        \n",
    "#         uncertainity_value=sum(1-li[ind])/(len(X_train[sent])+0.0005)\n",
    "#         uncertainity_value_all_predictions.append(uncertainity_value)\n",
    "#     return uncertainity_value_all_predictions\n",
    "\n",
    "\n",
    "def get_uncertainy_of_predicted_values(X_train,crf):\n",
    "\n",
    "    uncertainity_value_all_predictions=[]\n",
    "    for sent in range(len(X_train)):\n",
    "        uncertainity_value=0\n",
    "        kk=crf.predict_marginals_single(X_train[sent])\n",
    "        uncertainity_value=sum([1-max(kk[i].values()) for i in range(len(X_train[sent]))])        \n",
    "        uncertainity_value=uncertainity_value/(len(X_train[sent]))\n",
    "        uncertainity_value_all_predictions.append(uncertainity_value)\n",
    "\n",
    "    return uncertainity_value_all_predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_token_entropy(X_train,crf):\n",
    "\n",
    "    entropy_value_all_predictions=[]\n",
    "    for sent in range(len(X_train)):\n",
    "        sentence_entropy_value=0.0\n",
    "        kk=crf.predict_marginals_single(X_train[sent])\n",
    "        for i in range(len(X_train[sent])):\n",
    "            probs = np.array(list(kk[i].values()))\n",
    "            entropy = -np.sum(probs*np.log2(probs))\n",
    "            sentence_entropy_value += entropy\n",
    "            \n",
    "        entropy_value_all_predictions.append(sentence_entropy_value)\n",
    "\n",
    "    return entropy_value_all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lZyyKZGSDiwg"
   },
   "outputs": [],
   "source": [
    "# sent=1\n",
    "# kk=crf.predict_marginals_single(X_train[sent])\n",
    "# # 1-max(kk[0].values())\n",
    "\n",
    "# sum([1-max(kk[0].values()) for i in range(len(X_train[sent]))])\n",
    "\n",
    "# [1-max(kk[i].values()) for i in range(len(X_train[sent]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u2pgZEWAFd8N"
   },
   "outputs": [],
   "source": [
    "# sent=1\n",
    "# uncertainity_value=0\n",
    "# for words in range(len(X_train[sent])):\n",
    "#     print((1-crf.predict_marginals_single(X_train[sent])[words][prediction_of_classifier[sent][words]]))\n",
    "#     uncertainity_value=uncertainity_value+(1-crf.predict_marginals_single(X_train[sent])[words][prediction_of_classifier[sent][words]])\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dmPMSpUkHgXk"
   },
   "outputs": [],
   "source": [
    "# len(training_dataset_feats),len(training_dataset_labels)\n",
    "# m=np.array([1,3,2])\n",
    "# np.argsort(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "PAr0yyHMA7D-",
    "outputId": "4c5bc939-e032-423f-eca8-8fcf0215a5c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('The', 'DT', 'O'), ('president', 'NN', 'O'), (\"'s\", 'POS', 'O'), ('aircraft', 'NN', 'O'), ('has', 'VBZ', 'O'), ('received', 'VBN', 'O'), ('permission', 'NN', 'O'), ('to', 'TO', 'O'), ('pass', 'VB', 'O'), ('through', 'IN', 'O'), ('Israeli', 'JJ', 'I-MISC'), ('airspace', 'NN', 'O'), ('but', 'CC', 'O'), ('the', 'DT', 'O'), ('president', 'NN', 'O'), ('is', 'VBZ', 'O'), ('not', 'RB', 'O'), ('expected', 'VBN', 'O'), ('to', 'TO', 'O'), ('travel', 'VB', 'O'), ('to', 'TO', 'O'), ('the', 'DT', 'O'), ('West', 'NNP', 'I-LOC'), ('Bank', 'NNP', 'I-LOC'), ('before', 'IN', 'O'), ('Monday', 'NNP', 'O'), (',', ',', 'O'), ('\"', '\"', 'O'), ('Nabil', 'NNP', 'I-PER'), ('Abu', 'NNP', 'I-PER'), ('Rdainah', 'NNP', 'I-PER'), ('told', 'VBD', 'O'), ('Reuters', 'NNP', 'I-ORG'), ('.', '.', 'O')], [('8', 'CD', 'O'), ('-', ':', 'O'), ('Lindsay', 'NNP', 'I-PER'), ('Davenport', 'NNP', 'I-PER'), ('(', '(', 'O'), ('U.S.', 'NNP', 'I-LOC'), (')', ')', 'O'), ('beat', 'VB', 'O'), ('Henrietta', 'NNP', 'I-PER'), ('Nagyova', 'NNP', 'I-PER'), ('(', '(', 'O'), ('Slovakia', 'NNP', 'I-LOC'), (')', ')', 'O'), ('6-', 'CD', 'O'), ('0', 'CD', 'O'), ('6-4', 'CD', 'O')], [('Slovan', 'NNP', 'I-ORG'), ('Liberec', 'NNP', 'I-ORG'), ('3', 'CD', 'O'), ('1', 'CD', 'O'), ('1', 'CD', 'O'), ('1', 'CD', 'O'), ('5', 'CD', 'O'), ('4', 'CD', 'O'), ('4', 'CD', 'O')], [('Bonilla', 'NNP', 'I-PER'), ('has', 'VBZ', 'O'), ('21', 'CD', 'O'), ('RBI', 'NNP', 'I-MISC'), ('and', 'CC', 'O'), ('15', 'CD', 'O'), ('runs', 'NNS', 'O'), ('in', 'IN', 'O'), ('his', 'PRP$', 'O'), ('last', 'JJ', 'O'), ('20', 'CD', 'O'), ('games', 'NNS', 'O'), ('.', '.', 'O')], [('London', 'NNP', 'I-MISC'), ('Carnival', 'NNP', 'I-MISC'), ('ends', 'VBZ', 'O'), ('in', 'IN', 'O'), ('high', 'JJ', 'O'), ('spirits', 'NNS', 'O'), ('.', '.', 'O')], [('The', 'DT', 'O'), ('fourth-seeded', 'JJ', 'O'), ('Spaniard', 'NNP', 'I-MISC'), (',', ',', 'O'), ('who', 'WP', 'O'), ('is', 'VBZ', 'O'), ('tackling', 'VBG', 'O'), ('the', 'DT', 'O'), ('world', 'NN', 'O'), ('class', 'NN', 'O'), ('traffic', 'NN', 'O'), ('of', 'IN', 'O'), ('New', 'NNP', 'I-LOC'), ('York', 'NNP', 'I-LOC'), ('City', 'NNP', 'I-LOC'), ('as', 'IN', 'O'), ('a', 'DT', 'O'), ('warm-up', 'NN', 'O'), ('by', 'IN', 'O'), ('driving', 'VBG', 'O'), ('to', 'TO', 'O'), ('the', 'DT', 'O'), ('tennis', 'NN', 'O'), ('centre', 'NN', 'O'), ('for', 'IN', 'O'), ('her', 'PRP$', 'O'), ('matches', 'NNS', 'O'), (',', ',', 'O'), ('ran', 'VBD', 'O'), ('over', 'IN', 'O'), ('France', 'NNP', 'I-LOC'), (\"'s\", 'POS', 'O'), ('Nathalie', 'NNP', 'I-PER'), ('Tauziat', 'NNP', 'I-PER'), ('6-1', 'NNP', 'O'), ('6-3', 'CD', 'O'), ('on', 'IN', 'O'), ('Wednesday', 'NNP', 'O'), ('to', 'TO', 'O'), ('take', 'VB', 'O'), ('her', 'PRP$', 'O'), ('place', 'NN', 'O'), ('in', 'IN', 'O'), ('the', 'DT', 'O'), ('third', 'JJ', 'O'), ('round', 'NN', 'O'), ('.', '.', 'O')], [('1998', 'CD', 'O'), ('665M', 'CD', 'O'), ('840M', 'CD', 'O'), ('570M', 'CD', 'O')], [('Polish', 'JJ', 'I-MISC'), ('diplomat', 'NN', 'O'), ('denies', 'VBZ', 'O'), ('nurses', 'NNS', 'O'), ('stranded', 'VBN', 'O'), ('in', 'IN', 'O'), ('Libya', 'NNP', 'I-LOC'), ('.', '.', 'O')], [('The', 'DT', 'O'), ('airport', 'NN', 'O'), ('spokesman', 'NN', 'O'), ('said', 'VBD', 'O'), ('the', 'DT', 'O'), ('six', 'CD', 'O'), ('hijackers', 'NNS', 'O'), (',', ',', 'O'), ('who', 'WP', 'O'), ('police', 'NNS', 'O'), ('said', 'VBD', 'O'), ('were', 'VBD', 'O'), ('armed', 'JJ', 'O'), ('with', 'IN', 'O'), ('grenades', 'NNS', 'O'), ('and', 'CC', 'O'), ('possibly', 'RB', 'O'), ('other', 'JJ', 'O'), ('explosives', 'NNS', 'O'), (',', ',', 'O'), ('were', 'VBD', 'O'), ('believed', 'VBN', 'O'), ('to', 'TO', 'O'), ('be', 'VB', 'O'), ('Iraqi', 'JJ', 'I-MISC'), ('nationals', 'NNS', 'O'), ('.', '.', 'O')], [('Toronto-based', 'JJ', 'I-MISC'), ('Barrick', 'NNP', 'I-ORG'), (',', ',', 'O'), ('the', 'DT', 'O'), ('world', 'NN', 'O'), (\"'s\", 'POS', 'O'), ('third', 'JJ', 'O'), ('largest', 'JJS', 'O'), ('gold', 'NN', 'O'), ('producer', 'NN', 'O'), (',', ',', 'O'), ('sweetened', 'VBD', 'O'), ('its', 'PRP$', 'O'), ('July', 'NNP', 'O'), ('11', 'CD', 'O'), ('bid', 'NN', 'O'), ('to', 'TO', 'O'), ('C$', '$', 'I-MISC'), ('30', 'CD', 'O'), ('a', 'DT', 'O'), ('share', 'NN', 'O'), ('from', 'IN', 'O'), ('C$', '$', 'I-MISC'), ('27', 'CD', 'O'), ('on', 'IN', 'O'), ('August', 'NNP', 'O'), ('16', 'CD', 'O'), ('after', 'IN', 'O'), ('a', 'DT', 'O'), ('fresh', 'JJ', 'O'), ('batch', 'NN', 'O'), ('of', 'IN', 'O'), ('drill', 'NN', 'O'), ('results', 'NNS', 'O'), ('from', 'IN', 'O'), ('the', 'DT', 'O'), ('Pierina', 'NNP', 'I-LOC'), ('deposit', 'NN', 'O'), ('.', '.', 'O')], [('It', 'PRP', 'O'), ('said', 'VBD', 'O'), ('that', 'IN', 'O'), ('under', 'IN', 'O'), ('the', 'DT', 'O'), ('sale', 'NN', 'O'), ('agreement', 'NN', 'O'), (',', ',', 'O'), ('full', 'JJ', 'O'), ('financial', 'JJ', 'O'), ('details', 'NNS', 'O'), ('of', 'IN', 'O'), ('which', 'WDT', 'O'), ('were', 'VBD', 'O'), ('not', 'RB', 'O'), ('revealed', 'VBN', 'O'), (',', ',', 'O'), ('ISS', 'NNP', 'I-ORG'), ('would', 'MD', 'O'), ('acquire', 'VB', 'O'), ('a', 'DT', 'O'), ('25', 'CD', 'O'), ('percent', 'NN', 'O'), ('stake', 'NN', 'O'), ('in', 'IN', 'O'), ('Aaxis', 'NNP', 'I-ORG'), ('which', 'WDT', 'O'), ('would', 'MD', 'O'), ('become', 'VB', 'O'), ('an', 'DT', 'O'), ('associated', 'VBN', 'O'), ('company', 'NN', 'O'), ('within', 'IN', 'O'), ('the', 'DT', 'O'), ('ISS', 'NNP', 'I-ORG'), ('group', 'NN', 'O'), ('trading', 'NN', 'O'), ('under', 'IN', 'O'), ('the', 'DT', 'O'), ('ISS', 'NNP', 'I-ORG'), ('name', 'NN', 'O'), ('and', 'CC', 'O'), ('logo', 'NN', 'O'), ('.', '.', 'O')], [('15-10', 'JJ', 'O'), ('15-10', 'JJ', 'O'), ('15-10', 'JJ', 'O')], [('BASEBALL', 'NNP', 'O'), ('-', ':', 'O'), ('MAJOR', 'NNP', 'I-MISC'), ('LEAGUE', 'NNP', 'I-MISC'), ('STANDINGS', 'NNP', 'O'), ('AFTER', 'NNP', 'O'), ('SUNDAY', 'NNP', 'O'), (\"'S\", 'POS', 'O'), ('GAMES', 'NNS', 'O'), ('.', '.', 'O')], [('1.', 'CD', 'O'), ('Sorensen', 'NNP', 'I-PER'), ('11.20:33', 'CD', 'O')], [('NEW', 'NNP', 'I-LOC'), ('YORK', 'NNP', 'I-LOC'), ('1996-08-25', 'CD', 'O')], [('ISSUE', 'JJ', 'O'), ('Min', 'NN', 'O'), ('.', '.', 'O')], [('FK', 'NNP', 'I-ORG'), ('Teplice', 'NN', 'I-ORG'), ('3', 'CD', 'O'), ('1', 'CD', 'O'), ('1', 'CD', 'O'), ('1', 'CD', 'O'), ('3', 'CD', 'O'), ('4', 'CD', 'O'), ('4', 'CD', 'O')], [], [('--', ':', 'O'), ('John', 'NNP', 'I-PER'), ('Gilardi', 'NNP', 'I-PER'), (',', ',', 'O'), ('Frankfurt', 'NNP', 'I-ORG'), ('Newsroom', 'NNP', 'I-ORG'), (',', ',', 'O'), ('+49', 'CD', 'O'), ('69', 'CD', 'O'), ('756525', 'CD', 'O')], [('1.', 'CD', 'O'), ('Regina', 'NNP', 'I-PER'), ('Jacobs', 'NNP', 'I-PER'), ('(', '(', 'O'), ('U.S.', 'NNP', 'I-LOC'), (')', ')', 'O'), ('4', 'CD', 'O'), ('minutes', 'NNS', 'O'), ('01.77', 'CD', 'O'), ('seconds', 'NNS', 'O')]]\n",
      "(14001,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i=3\n",
    "n=12\n",
    "iter_count=0\n",
    "sub_sample=20\n",
    "F_SCORE_TEST=[]\n",
    "NUMBER_OF_DATA=[]\n",
    "while(len(X_train)):\n",
    "    if iter_count>=120:\n",
    "        sub_sample=200\n",
    "\n",
    "    uncertainity_value_all_predictions=get_total_token_entropy(X_train,crf)\n",
    "    indices_of_most_confused_sentences = np.argsort(uncertainity_value_all_predictions)[-sub_sample:]\n",
    "    print([unannotated_dataset[i] for i in indices_of_most_confused_sentences])\n",
    "    training_dataset_feats.extend(X_train[indices_of_most_confused_sentences])\n",
    "    training_dataset_labels.extend(train_labels[indices_of_most_confused_sentences])\n",
    "\n",
    "    X_train = np.delete(X_train, indices_of_most_confused_sentences)\n",
    "    train_labels = np.delete(train_labels, indices_of_most_confused_sentences)\n",
    "    print(X_train.shape)\n",
    "    crf.fit(training_dataset_feats,training_dataset_labels)\n",
    "    prediction_of_classifier=crf.predict(X_train)  \n",
    "\n",
    "    test_pred = crf.predict(test_feats)\n",
    "    NUMBER_OF_DATA.append(len(training_dataset_feats))\n",
    "    fscore_value=sklearn_crfsuite.metrics.flat_f1_score(test_labels, test_pred, average=\"macro\")\n",
    "    F_SCORE_TEST.append(fscore_value)\n",
    "    print(\"n = \",NUMBER_OF_DATA[-1],\"f1 score \",fscore_value)\n",
    "    \n",
    "    with open(\"TTE_eng.txt\", \"a\") as f:\n",
    "        f.write(str(NUMBER_OF_DATA[-1]) + \" \" + str(fscore_value)+\"\\n\")\n",
    "\n",
    "    i=i+1\n",
    "    iter_count=iter_count+1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fCIhjB6yRFEg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jP24bwafWLW6"
   },
   "source": [
    "#### RANDOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tqfvpVXZWOB7"
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train_random_s = X_train_random.copy()\n",
    "train_labels_random_s = train_labels_random.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aWufrRv_VznX"
   },
   "outputs": [],
   "source": [
    "initial_size_of_training_set = 20\n",
    "\n",
    "# training_dataset_feats = {}\n",
    "# training_dataset_labels = {}\n",
    "\n",
    "# indices=np.random.randint(0, X_train_random_s.shape[0], size=initial_size_of_training_set)\n",
    "# training_dataset_feats = list(X_train_random_s[indices])\n",
    "# training_dataset_labels =  list(train_labels_random_s[indices])\n",
    "\n",
    "training_dataset_feats=initial_training_dataset_feats.copy()\n",
    "training_dataset_labels=initial_training_dataset_labels.copy()\n",
    "\n",
    "X_train = X_train_random_s\n",
    "train_labels = train_labels_random_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "dDNC2vJcVzoM",
    "outputId": "770f27fe-f826-44dd-b48c-6bef20e4f8d4"
   },
   "outputs": [],
   "source": [
    "len(training_dataset_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A3Yh32PdVzoU"
   },
   "outputs": [],
   "source": [
    "\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s4xdHp8iVzoi"
   },
   "outputs": [],
   "source": [
    "# prediction_of_classifier[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nRdyGm8qVzoy"
   },
   "outputs": [],
   "source": [
    "# crf.predict_marginals_single(X_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "c9rzpeg3VzpE",
    "outputId": "5a65430b-f791-475d-8a04-9ca202ee598b"
   },
   "outputs": [],
   "source": [
    "\n",
    "labels_predicted_for_unannotated_data = []\n",
    "\n",
    "print(\"SIZE OF TRAINING DATASET \",len(training_dataset_feats))\n",
    "crf.fit(training_dataset_feats,training_dataset_labels)\n",
    "prediction_of_classifier=crf.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "yUIxbg4KVzpd",
    "outputId": "14981dd8-597b-43e0-8414-61cd885be32d"
   },
   "outputs": [],
   "source": [
    "len(training_dataset_feats),len(training_dataset_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "TLv5vPQYeO8x",
    "outputId": "a47d14c6-f07f-48f9-efa9-1e0ad779a240"
   },
   "outputs": [],
   "source": [
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ZpnoYo9LVzpk",
    "outputId": "ccc52134-9f0d-4e38-efcb-9236ae9c97a7"
   },
   "outputs": [],
   "source": [
    "i=3\n",
    "F_SCORE_TEST_RANDOM=[]\n",
    "NUMBER_OF_DATA_RANDOM=[]\n",
    "sub_sample=20\n",
    "iter_count=0\n",
    "while(len(X_train)):\n",
    "    if iter_count>=120:\n",
    "        sub_sample=200\n",
    "    ind_random=np.random.randint(0,len(X_train), size=sub_sample)\n",
    "\n",
    "  \n",
    "    training_dataset_feats.extend(X_train[ind_random])\n",
    "    training_dataset_labels.extend(train_labels[ind_random])\n",
    "\n",
    "    X_train = np.delete(X_train, ind_random)\n",
    "    train_labels = np.delete(train_labels, ind_random)\n",
    "    print(X_train.shape)\n",
    "\n",
    "    crf.fit(training_dataset_feats,training_dataset_labels)\n",
    "    # prediction_of_classifier=crf.predict(X_train)  \n",
    "\n",
    "    test_pred = crf.predict(test_feats)\n",
    "    NUMBER_OF_DATA_RANDOM.append(len(training_dataset_feats))\n",
    "    fscore_value=sklearn_crfsuite.metrics.flat_f1_score(test_labels, test_pred, average=\"macro\")\n",
    "    F_SCORE_TEST_RANDOM.append(fscore_value)\n",
    "    print(\"n = \",NUMBER_OF_DATA_RANDOM[-1],\"f1 score \",fscore_value)\n",
    "    \n",
    "    with open(\"TTE_english.txt\", \"a\") as f:\n",
    "        f.write(str(len(training_dataset_feats[0])) + \" \" + str(fscore_value)+\"\\n\")\n",
    "\n",
    "\n",
    "    i=i+1\n",
    "    iter_count=iter_count+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tTipfa5r2Eco"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "flZiDWnfrE7b",
    "outputId": "fea227c6-17b1-4ebb-8ea6-112c5715f275"
   },
   "outputs": [],
   "source": [
    "plt.plot(NUMBER_OF_DATA_RANDOM,F_SCORE_TEST_RANDOM,label='Random')\n",
    "plt.plot(NUMBER_OF_DATA,F_SCORE_TEST,label='Uncertainity Sampling') \n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AXukyDyrXdws"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Peter', 'Blackburn']\n"
     ]
    }
   ],
   "source": [
    "train_ua = ConllCorpusReader('./CoNLL-2003', 'eng.train', ['words'])\n",
    "train_ua = list(train_ua.sents())\n",
    "print(train_ua[2])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "uncertainity_sampling_LC.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
