{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x8TaSlPS1bOh"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2002\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import Perceptron, SGDClassifier, RidgeClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import sklearn_crfsuite\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import nltk\n",
    "from nltk.classify import MaxentClassifier\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('conll2002')\n",
    "\n",
    "regex = re.compile(\n",
    "        r'^(?:http|ftp)s?://'  # http:// or https://\n",
    "        r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|'  # domain...\n",
    "        r'localhost|'  # localhost...\n",
    "        r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})'  # ...or ip\n",
    "        r'(?::\\d+)?'  # optional port\n",
    "        r'(?:/?|[/?]\\S+)$', re.IGNORECASE)\n",
    "\n",
    "def wordshape(text):\n",
    "\n",
    "    t1 = re.sub('[A-Z]', 'X',text)\n",
    "    t2 = re.sub('[a-z]', 'x', t1)\n",
    "    return re.sub('[0-9]', 'd', t2)\n",
    "\n",
    "def getfeats(word, o):\n",
    "    \"\"\" This takes the word in question and\n",
    "    the offset with respect to the instance\n",
    "    word \"\"\"\n",
    "    features = [\n",
    "        (str(o) + 'word', word)\n",
    "        # TODO: add more features here.\n",
    "    ]\n",
    "    return features\n",
    "\n",
    "def gettag(tag, o):\n",
    "    features = [ (str(o) +\"tag\", tag) ]\n",
    "    return features\n",
    "\n",
    "def gethyphen(word, o):\n",
    "    if('-' in word):\n",
    "        features = [(str(o) +\"hyphen\", 1)]\n",
    "    else:\n",
    "        features = [(str(o) +\"hyphen\", 0)]\n",
    "    return features\n",
    "    \n",
    "def capletter(word, o):\n",
    "    if(word[0].isupper):\n",
    "        features = [(str(o) +\"first_upper\", 1)]\n",
    "    else:\n",
    "        features = [(str(o) +\"first_upper\", 0)]\n",
    "    return features\n",
    "\n",
    "def noun_suffix(word, o):\n",
    "    if(word.endswith('o') or word.endswith('or') or word.endswith('a') or word.endswith('ora')):\n",
    "        features = [(str(o) +\"common_suffix\", 1)]\n",
    "    else:\n",
    "        features = [(str(o) +\"common_suffix\", 0)]\n",
    "    return features\n",
    "\n",
    "def get_wordshape(word, o):\n",
    "    feature = [(str(o) +\"word_shape\", wordshape(word))]\n",
    "    return feature\n",
    "\n",
    "def all_upper(word, o):\n",
    "    if(word.isupper()):\n",
    "        return [(str(o) +\"all_upper\", 1)]\n",
    "    else:\n",
    "        return [(str(o) +\"all_upper\", 0)]\n",
    "\n",
    "def all_lower(word, o):\n",
    "    if(word.islower()):\n",
    "        return [(str(o) +\"all_lower\", 1)]\n",
    "    else:\n",
    "        return [(str(o) +\"all_lower\", 0)]\n",
    "\n",
    "def has_apostrophe(word, o):\n",
    "    if(\"'\" in word):\n",
    "        return [(str(o) +\"apostrophe\", 1)]\n",
    "    else:\n",
    "        return [(str(o) +\"apostrophe\", 0)]\n",
    "\n",
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def special_characters(word, o):\n",
    "    if(isEnglish(word)):\n",
    "        return [(str(o) +\"special_characters\", 0)]\n",
    "    else:\n",
    "        return [(str(o) +\"special_characters\", 1)]\n",
    "\n",
    "def onlynum(word, o):\n",
    "    if(word.isdigit()):\n",
    "        return [(str(o) +\"onlynum\", 1)]\n",
    "    else:\n",
    "        return [(str(o) +\"onlynum\", 0)]\n",
    "\n",
    "def contains_num(word, o):\n",
    "    if(any(char.isdigit() for char in word)):\n",
    "        return [(str(o) +\"contains_num\", 1)]\n",
    "    else:\n",
    "        return [(str(o) +\"contains_num\", 0)]\n",
    "\n",
    "def ending_fullstop(word, o):\n",
    "    if(word[-1] == '.'):\n",
    "        return [(str(o) +\"fullstop\", 1)]\n",
    "    else:\n",
    "        return [(str(o) +\"fullstop\", 0)]\n",
    "\n",
    "def minlen(word, o):\n",
    "    if(len(word)>=2):\n",
    "        return [(str(o) +\"minlen\", 1)]\n",
    "    else:\n",
    "        return [(str(o) +\"minlen\", 0)]\n",
    "\n",
    "def punctuation(word, o):\n",
    "    for i in word: \n",
    "      if i in string.punctuation: \n",
    "        return [(str(o) +\"punctuation\", 1)]\n",
    "    return [(str(o) +\"punctuation\", 0)]\n",
    "\n",
    "def all_punctuation(word, o):\n",
    "    count = 0\n",
    "    for i in word: \n",
    "      if i in string.punctuation: \n",
    "        count = count +1\n",
    "    if(count == len(word)):\n",
    "        return [(str(o) +\"punctuation\", 1)]\n",
    "    else:\n",
    "        return [(str(o) +\"punctuation\", 0)]\n",
    "\n",
    "def is_stopword(word, o):\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    if(word in stop_words):\n",
    "        return([(str(o) +\"is_stop\", 1)])\n",
    "    else:\n",
    "        return([(str(o) +\"is_stop\", 0)])\n",
    "\n",
    "\n",
    "def isRomanNumeral(word, o):\n",
    "    numeral = word.upper()\n",
    "    validRomanNumerals = [\"M\", \"D\", \"C\", \"L\", \"X\", \"V\", \"I\"]\n",
    "    for letters in numeral:\n",
    "        if letters not in validRomanNumerals:\n",
    "            return ([(str(o) +\"is_roman\", 0)])\n",
    "\n",
    "    return ([(str(o) + \"is_roman\", 1)])\n",
    "\n",
    "\n",
    "def contains_dots(word, o):\n",
    "    if word.find('.')==-1:\n",
    "        return ([(str(o) + \"has_dot\", 0)])\n",
    "\n",
    "    return ([(str(o) + \"has_dot\", 1)])\n",
    "\n",
    "\n",
    "def single_char(word, o):\n",
    "    if len(word)==1:\n",
    "        return ([(str(o) + \"is_char\", 1)])\n",
    "\n",
    "    return ([(str(o) + \"is_char\", 0)])\n",
    "\n",
    "def is_url(word, o):\n",
    "    if re.match(regex, word) is not None:\n",
    "        return ([(str(o) + \"is_url\", 1)])\n",
    "    return ([(str(o) + \"is_url\", 0)])\n",
    "\n",
    "\n",
    "def word2features(sent, i):\n",
    "    \"\"\" The function generates all features\n",
    "    for the word at position i in the\n",
    "    sentence.\"\"\"\n",
    "    features = []\n",
    "    # the window around the token\n",
    "    for o in [-4,-3, -2,-1,0,1,2, 3, 4]:\n",
    "        if i+o >= 0 and i+o < len(sent):\n",
    "            word = sent[i+o][0]\n",
    "            tag = sent[i+o][1]\n",
    "            featlist = getfeats(word, o)\n",
    "            features.extend(featlist)\n",
    "            featlist = gettag(tag, o)\n",
    "            features.extend(featlist)\n",
    "\n",
    "            featlist = gethyphen(word, o)\n",
    "            features.extend(featlist)\n",
    "\n",
    "            featlist = capletter(word, o)\n",
    "            features.extend(featlist)\n",
    "\n",
    "            featlist = noun_suffix(word, o)\n",
    "            features.extend(featlist)\n",
    "\n",
    "            featlist = get_wordshape(word, o)\n",
    "            features.extend(featlist)\n",
    "\n",
    "            featlist = all_upper(word, o)\n",
    "            features.extend(featlist)\n",
    "\n",
    "            featlist = all_lower(word, o)\n",
    "            features.extend(featlist)\n",
    "\n",
    "            featlist = has_apostrophe(word, o)\n",
    "            features.extend(featlist)\n",
    "\n",
    "            featlist = special_characters(word, o)\n",
    "            features.extend(featlist)\n",
    "\n",
    "            featlist = onlynum(word, o)\n",
    "            features.extend(featlist)\n",
    "\n",
    "            featlist = contains_num(word, o)\n",
    "            features.extend(featlist)\n",
    "\n",
    "            featlist = ending_fullstop(word, o)\n",
    "            features.extend(featlist)\n",
    "\n",
    "            featlist = isRomanNumeral(word, o)\n",
    "            features.extend(featlist)\n",
    "\n",
    "            featlist = contains_dots(word, o)\n",
    "            features.extend(featlist)\n",
    "\n",
    "            featlist = single_char(word, o)\n",
    "            features.extend(featlist)\n",
    "\n",
    "            featlist = is_url(word, o)\n",
    "            features.extend(featlist)\n",
    "\n",
    "    \n",
    "    word = sent[i][0]\n",
    "    tag = sent[i][1]\n",
    "\n",
    "    features.extend([(\"word_lower\", word.lower())])\n",
    "\n",
    "    features.extend([(\"word_len\", len(word))])\n",
    "\n",
    "    if (i == 0):\n",
    "        features.extend([(\"firstword\", 1)])\n",
    "    else:\n",
    "        features.extend([(\"firstword\", 0)])\n",
    "\n",
    "    features.extend([(\"bias\", 1)])\n",
    "    \n",
    "    return dict(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rj-1nnfV1cpb"
   },
   "outputs": [],
   "source": [
    "train = ConllCorpusReader('CoNLL-2003', 'eng.train', ['words', 'pos', 'ignore', 'chunk'])\n",
    "dev = ConllCorpusReader('CoNLL-2003', 'eng.testa', ['words', 'pos', 'ignore', 'chunk'])\n",
    "test = ConllCorpusReader('CoNLL-2003', 'eng.testb', ['words', 'pos', 'ignore', 'chunk'])\n",
    "\n",
    "train_sents = list(train.iob_sents())\n",
    "dev_sents = list(dev.iob_sents())\n",
    "test_sents = list(test.iob_sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gnjuyDFKn3Pb"
   },
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "train_feats = []\n",
    "train_labels = []\n",
    "\n",
    "for sent in train_sents:\n",
    "    train_feats = []\n",
    "    train_labels = []\n",
    "    for i in range(len(sent)):\n",
    "        feats = word2features(sent, i)\n",
    "        train_feats.append(feats)\n",
    "        train_labels.append(sent[i][-1])\n",
    "    X_train.append(train_feats)\n",
    "    y_train.append(train_labels)\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "test_feats = []\n",
    "test_labels = []\n",
    "\n",
    "for sent in test_sents:\n",
    "    test_feats = []\n",
    "    test_labels = []\n",
    "    for i in range(len(sent)):\n",
    "        feats = word2features(sent, i)\n",
    "        test_feats.append(feats)\n",
    "        test_labels.append(sent[i][-1])\n",
    "    X_test.append(test_feats)\n",
    "    y_test.append(test_labels)\n",
    "\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "train_labels = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "test_labels = np.concatenate(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Jki8KCS2Cvi"
   },
   "outputs": [],
   "source": [
    "no_of_learners = 3\n",
    "initial_size_of_training_set = 20\n",
    "\n",
    "training_dataset_feats = {}\n",
    "training_dataset_labels = {}\n",
    "\n",
    "for learner_i in range(no_of_learners):\n",
    "    randomly_sampled_data = np.random.randint(0, X_train.shape[0], size=initial_size_of_training_set)\n",
    "\n",
    "    training_dataset_feats[learner_i] = list(X_train[randomly_sampled_data])\n",
    "    training_dataset_labels[learner_i] =  list(train_labels[randomly_sampled_data])\n",
    "\n",
    "    X_train = np.delete(X_train, randomly_sampled_data)\n",
    "    train_labels = np.delete(train_labels, randomly_sampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zVy9RAJh3EYJ"
   },
   "outputs": [],
   "source": [
    "for i in range(120):\n",
    "\n",
    "    labels_predicted_for_unannotated_data = []\n",
    "\n",
    "    for learner_i in range(no_of_learners):\n",
    "\n",
    "        model = sklearn_crfsuite.CRF()\n",
    "        model.fit(training_dataset_feats[learner_i], training_dataset_labels[learner_i])\n",
    "\n",
    "        prediction_of_classifier = model.predict(X_train)\n",
    "        \n",
    "        labels_predicted_for_unannotated_data.append(prediction_of_classifier)\n",
    "\n",
    "    no_of_sentences_in_unannotated_dataset = len(labels_predicted_for_unannotated_data[0])\n",
    "    entropy_of_sentences = []\n",
    "\n",
    "    for i in range(no_of_sentences_in_unannotated_dataset):\n",
    "        sentence_wise_labels = []\n",
    "        for j in range(no_of_learners):\n",
    "            sentence_wise_labels.append(labels_predicted_for_unannotated_data[j][i])\n",
    "\n",
    "        sentence_wise_labels = np.array(sentence_wise_labels).T\n",
    "\n",
    "        entropy_of_sentence = []\n",
    "\n",
    "        for token_labels in sentence_wise_labels:\n",
    "            entropy_of_token = 0\n",
    "            count_of_labels = Counter(token_labels)\n",
    "\n",
    "            for count in count_of_labels.values():\n",
    "                entropy_of_token += count/no_of_learners * math.log(count/no_of_learners)\n",
    "\n",
    "            entropy_of_token = -1/math.log(no_of_learners) * entropy_of_token\n",
    "            entropy_of_sentence.append(entropy_of_token)\n",
    "        \n",
    "        entropy_of_sentences.append(np.array(entropy_of_sentence).mean())\n",
    "\n",
    "    entropy_of_sentences = np.nan_to_num(entropy_of_sentences)\n",
    "\n",
    "    indices_of_most_confused_sentences = np.argsort(entropy_of_sentences)[-20:]\n",
    "\n",
    "    test_pred = model.predict(X_test)\n",
    "    current_f1_score = f1_score(test_labels, np.concatenate(test_pred), average='macro')\n",
    "\n",
    "    print(\"n = \",len(training_dataset_feats[0]),\"f1 score \",current_f1_score)\n",
    "\n",
    "    with open(\"results_530_MS2.txt\", \"a\") as f:\n",
    "        f.write(str(len(training_dataset_feats[0])) + \" \" + str(current_f1_score)+\"\\n\")\n",
    "\n",
    "\n",
    "    for learner_i in range(no_of_learners):\n",
    "        training_dataset_feats[learner_i].extend(X_train[indices_of_most_confused_sentences])\n",
    "        training_dataset_labels[learner_i].extend(train_labels[indices_of_most_confused_sentences])\n",
    "\n",
    "    X_train = np.delete(X_train, indices_of_most_confused_sentences)\n",
    "    train_labels = np.delete(train_labels, indices_of_most_confused_sentences)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "published_baseline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
